{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PreprocessAmazonBooks.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOKP6D4KQl6AkQnvLg9hNuL"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"eeAqlfQa7Z3J","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":137},"outputId":"31bf61e2-8f41-4565-b1e1-6f853f828ea9","executionInfo":{"status":"ok","timestamp":1581536655757,"user_tz":420,"elapsed":15094,"user":{"displayName":"Vinodhini Sivakami Duraisamy","photoUrl":"","userId":"08629147319952024849"}}},"source":["from google.colab import drive\n","drive.mount(\"/gdrive\")\n","%cd /gdrive"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /gdrive\n","/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4790lDLM7k53","colab_type":"code","colab":{}},"source":["import string\n","import re\n","from nltk.corpus import stopwords\n","import networkx\n","import pandas as pd"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rq4Tx5lY77l3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":67},"outputId":"b1a09e8b-13de-42d1-a771-777a41b5a58d","executionInfo":{"status":"ok","timestamp":1581536775772,"user_tz":420,"elapsed":478,"user":{"displayName":"Vinodhini Sivakami Duraisamy","photoUrl":"","userId":"08629147319952024849"}}},"source":["import nltk\n","nltk.download('stopwords')"],"execution_count":7,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"NKfAjfJ77oUI","colab_type":"code","colab":{}},"source":["# open file to read amazon product metadata \n","# Dataset: http://snap.stanford.edu/data/amazon-meta.html\n","fhr = open('/gdrive/My Drive/CIS_508/Colab Notebooks/CIS_509/Asmt3/amazon-meta.txt', 'r', encoding='utf-8', errors='ignore')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"fGnkKrjU74Ge","colab_type":"code","colab":{}},"source":["# initialize a nested product dictionary that will hold cleaned up amazon product data\n","# key = ASIN; value = MetaData associated with ASIN\n","amazonProductsND = {}\n","\n","# read the data from the amazon-meta file;\n","# populate amazonProductsND nested dicitonary;\n","(Id, ASIN, Title, Categories, Group, Copurchased, SalesRank, TotalReviews, AvgRating, DegreeCentrality, ClusteringCoeff) = \\\n","    (\"\", \"\", \"\", \"\", \"\", \"\", 0, 0, 0.0, 0, 0.0)\n","for line in fhr:\n","    line = line.strip()\n","    # a product block started\n","    if(line.startswith(\"Id\")):\n","        Id = line[3:].strip()\n","    elif(line.startswith(\"ASIN\")):\n","        ASIN = line[5:].strip()\n","    elif(line.startswith(\"title\")):\n","        Title = line[6:].strip()\n","        Title = ' '.join(Title.split())\n","    elif(line.startswith(\"group\")):\n","        Group = line[6:].strip()\n","    elif(line.startswith(\"salesrank\")):\n","        SalesRank = line[10:].strip()\n","    elif(line.startswith(\"similar\")):\n","        ls = line.split()\n","        Copurchased = ' '.join([c for c in ls[2:]])\n","    elif(line.startswith(\"categories\")):\n","        ls = line.split()\n","        Categories = ' '.join((fhr.readline()).lower() for i in range(int(ls[1].strip())))\n","        Categories = re.compile('[%s]' % re.escape(string.digits+string.punctuation)).sub(' ', Categories)\n","        Categories = ' '.join(set(Categories.split())-set(stopwords.words(\"english\")))\n","    elif(line.startswith(\"reviews\")):\n","        ls = line.split()\n","        TotalReviews = ls[2].strip()\n","        AvgRating = ls[7].strip()\n","    # a product block ended\n","    # write out fields to amazonProductsND Dictionary\n","    elif (line==\"\"):\n","        try:\n","            MetaData = {}\n","            if (ASIN != \"\"):\n","                amazonProductsND[ASIN]=MetaData\n","            MetaData['Id'] = Id            \n","            MetaData['Title'] = Title\n","            MetaData['Categories'] = ' '.join(set(Categories.split()))\n","            MetaData['Group'] = Group\n","            MetaData['Copurchased'] = Copurchased\n","            MetaData['SalesRank'] = int(SalesRank)\n","            MetaData['TotalReviews'] = int(TotalReviews)\n","            MetaData['AvgRating'] = float(AvgRating)\n","            MetaData['DegreeCentrality'] = DegreeCentrality\n","            MetaData['ClusteringCoeff'] = ClusteringCoeff\n","        except NameError:\n","            continue\n","        (Id, ASIN, Title, Categories, Group, Copurchased, SalesRank, TotalReviews, AvgRating, DegreeCentrality, ClusteringCoeff) = \\\n","            (\"\", \"\", \"\", \"\", \"\", \"\", 0, 0, 0.0, 0, 0.0)\n","fhr.close()\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rx4_H8Ax7OmK","colab_type":"code","colab":{}},"source":["\n","# create books-specific dictionary exclusively for books\n","amazonBooksND = {}\n","for asin,metadata in amazonProductsND.items():\n","    if (metadata['Group']=='Book'):\n","        amazonBooksND[asin] = amazonProductsND[asin]\n","    \n","# remove any copurchased items from copurchase list \n","# if we don't have metadata associated with it \n","for asin, metadata in amazonBooksND.items(): \n","    amazonBooksND[asin]['Copurchased'] = \\\n","        ' '.join([cp for cp in metadata['Copurchased'].split() \\\n","            if cp in amazonBooksND.keys()])\n","\n","# create a book copurchase graph for analysis\n","# where the graph nodes are book ASINs;\n","# and graph edge exists if two books were copurchased,\n","# with edge weight being a measure of category similarity between ASINs\n","copurchaseGraph = networkx.Graph()\n","for asin,metadata in amazonBooksND.items():\n","    copurchaseGraph.add_node(asin)\n","    for a in metadata['Copurchased'].split():\n","        copurchaseGraph.add_node(a.strip())\n","        similarity = 0        \n","        n1 = set((amazonBooksND[asin]['Categories']).split())\n","        n2 = set((amazonBooksND[a]['Categories']).split())\n","        n1In2 = n1 & n2\n","        n1Un2 = n1 | n2\n","        if (len(n1Un2)) > 0:\n","            similarity = round(len(n1In2)/len(n1Un2),2)\n","        copurchaseGraph.add_edge(asin, a.strip(), weight=similarity)\n","\n","# get degree centrality and clustering coefficients \n","# of each ASIN and add it to amazonBooks metadata\n","dc = networkx.degree(copurchaseGraph)\n","for asin in networkx.nodes(copurchaseGraph):\n","    metadata = amazonBooksND[asin]\n","    metadata['DegreeCentrality'] = int(dc[asin])\n","    ego = networkx.ego_graph(copurchaseGraph, asin, radius=1)\n","    metadata['ClusteringCoeff'] = networkx.clustering(ego, asin)\n","    amazonBooksND[asin] = metadata\n","\n","# convert amazonBooks metadata to pandas dataframe and drop redundant columns\n","amazonBooks = pd.DataFrame(amazonBooksND).T\n","amazonBooks.drop(columns=['Copurchased', 'Group'], axis=1, inplace=True)\n","\n","# write amazonBooks dataframe to csv file\n","amazonBooks.to_csv('/gdrive/My Drive/CIS_508/Colab Notebooks/CIS_509/Asmt3/amazon-books.csv', index=True, header=True)\n","\n","# write copurchaseGraph data to file\n","fhw=open(\"/gdrive/My Drive/CIS_508/Colab Notebooks/CIS_509/Asmt3/amazon-books-copurchase.edgelist\",'wb')\n","networkx.write_weighted_edgelist(copurchaseGraph, fhw)\n","fhw.close()"],"execution_count":0,"outputs":[]}]}